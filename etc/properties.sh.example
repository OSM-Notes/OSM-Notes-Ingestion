#!/bin/bash

# Properties for OSM-Notes-Ingestion - EXAMPLE FILE
# Copy this file to properties.sh for your local configuration:
#   cp etc/properties.sh.example etc/properties.sh
# Then edit etc/properties.sh with your database credentials.
#
# Author: Andres Gomez (AngocA)
# Version: 2025-12-13

# Database configuration
# All database connections must be controlled by this file.
# Environment variables are ignored to ensure consistency.
# shellcheck disable=SC2034
if [[ -z "${DBNAME:-}" ]]; then
 DBNAME="notes"
fi
# shellcheck disable=SC2034
if [[ -z "${DB_USER:-}" ]]; then
 DB_USER="myuser"
fi

# Email configuration for reports.
declare EMAILS="${EMAILS:-username@domain.com}"

# Alert configuration for failed executions.
# Email address for immediate failure alerts.
# shellcheck disable=SC2034
declare ADMIN_EMAIL="${ADMIN_EMAIL:-root@localhost}"

# Enable/disable email alerts on failures.
# Set to "true" to send immediate email alerts when critical errors occur.
# Set to "false" to only create failed execution marker files without sending alerts.
# shellcheck disable=SC2034
declare SEND_ALERT_EMAIL="${SEND_ALERT_EMAIL:-false}"

# OpenStreetMap API configuration.
# shellcheck disable=SC2034
declare OSM_API="${OSM_API:-https://api.openstreetmap.org/api/0.6}"

# OpenStreetMap Planet dump URL.
# shellcheck disable=SC2034
if [[ -z "${PLANET:-}" ]]; then
 declare -r PLANET="https://planet.openstreetmap.org"
fi

# Overpass interpreter URL. Used to download the countries and maritime boundaries.
# shellcheck disable=SC2034
if [[ -z "${OVERPASS_INTERPRETER:-}" ]]; then
 declare -r OVERPASS_INTERPRETER="https://overpass-api.de/api/interpreter"
fi

# Overpass fallback and validation configuration.
# Comma-separated list of interpreter endpoints. The first one is primary.
# Example:
#   export OVERPASS_ENDPOINTS="https://overpass-api.de/api/interpreter,https://overpass.kumi.systems/api/interpreter"
declare OVERPASS_ENDPOINTS="${OVERPASS_ENDPOINTS:-${OVERPASS_INTERPRETER}}"

# Max retries per endpoint for a single boundary download attempt.
declare OVERPASS_RETRIES_PER_ENDPOINT="${OVERPASS_RETRIES_PER_ENDPOINT:-7}"

# Base backoff (seconds) between retries within the same endpoint (exponential).
declare OVERPASS_BACKOFF_SECONDS="${OVERPASS_BACKOFF_SECONDS:-20}"

# Continue processing other boundaries on Overpass JSON validation errors.
declare CONTINUE_ON_OVERPASS_ERROR="${CONTINUE_ON_OVERPASS_ERROR:-true}"

# Overpass retry configuration when CONTINUE_ON_OVERPASS_ERROR=true
# These settings balance speed with success rate for boundary downloads
# Max retries per endpoint when continuing on errors (increased from 1 to 3
# for better success rate with temporary Overpass API issues)
declare OVERPASS_CONTINUE_MAX_RETRIES_PER_ENDPOINT="${OVERPASS_CONTINUE_MAX_RETRIES_PER_ENDPOINT:-3}"

# Base delay (seconds) between retries when continuing on errors
# Increased from 5 to 12 seconds to give Overpass API more time to recover
# from temporary issues like 504 Gateway Timeout
declare OVERPASS_CONTINUE_BASE_DELAY="${OVERPASS_CONTINUE_BASE_DELAY:-12}"

# Download validation retries when continuing on errors
# Number of attempts to download and validate JSON for each boundary
# Increased from 1 to 3 to improve success rate for temporary failures
declare OVERPASS_CONTINUE_VALIDATION_RETRIES="${OVERPASS_CONTINUE_VALIDATION_RETRIES:-3}"

# JSON validator command (must support: jq -e .).
declare JSON_VALIDATOR="${JSON_VALIDATOR:-jq}"

# Generic download User-Agent applied to all HTTP requests when supported.
# Recommended format: ProjectName/Version (+project_url; contact: email)
# Defaults to project identity if not provided.
# IMPORTANT: You MUST replace 'your-email@domain.com' with your actual email
# address. This follows OpenStreetMap best practices and allows server
# administrators to contact you if there are issues with your requests.
if [[ -z "${DOWNLOAD_USER_AGENT:-}" ]]; then
 # Do not break lines; keep UA in one line for header correctness
 DOWNLOAD_USER_AGENT="OSM-Notes-Ingestion/2025-10-30 (+https://github.com/osmlatam/OSM-Notes-Ingestion; contact: your-email@domain.com)"
fi

# Processing configuration.
# Quantity of notes to process per loop, to get the location of the note.
# shellcheck disable=SC2034
if [[ -z "${LOOP_SIZE:-}" ]]; then
 declare -r LOOP_SIZE="10000"
fi

# Maximum number of notes to download from the API.
# shellcheck disable=SC2034
if [[ -z "${MAX_NOTES:-}" ]]; then
 declare -r MAX_NOTES="10000"
fi

# Parallel processing configuration.
# Number of threads to use in parallel processing.
# It should be less than the number of cores of the server.
# shellcheck disable=SC2034
declare MAX_THREADS="4"

# Minimum number of notes to enable parallel processing.
# If the number of notes is less than this threshold, processing will be sequential.
# This helps avoid the overhead of parallelization for small datasets.
# shellcheck disable=SC2034
if [[ -z "${MIN_NOTES_FOR_PARALLEL:-}" ]]; then
 declare -r MIN_NOTES_FOR_PARALLEL="10"
fi

# Set MAX_THREADS based on available cores, leaving some for system
# This prevents system saturation and allows OS, PostgreSQL, and other processes to run
if command -v nproc > /dev/null 2>&1; then
 TOTAL_CORES=$(nproc)

 # Leave at least 2 cores free for system and database
 if [[ "${TOTAL_CORES}" -gt 4 ]]; then
  MAX_THREADS=$((TOTAL_CORES - 2))
 elif [[ "${TOTAL_CORES}" -gt 2 ]]; then
  MAX_THREADS=$((TOTAL_CORES - 1)) # Leave at least 1 core free
 else
  MAX_THREADS=1 # Use only 1 thread on systems with 1-2 cores
 fi

 # Limit to reasonable values for production
 if [[ "${MAX_THREADS}" -gt 16 ]]; then
  MAX_THREADS=16
 fi
else
 MAX_THREADS=4
fi

# Download threads configuration (for boundary downloads from Overpass)
# Separate from MAX_THREADS to control Overpass API load
# Lower values (4-6) prevent rate limiting issues
# shellcheck disable=SC2034
if [[ -z "${DOWNLOAD_MAX_THREADS:-}" ]]; then
 declare DOWNLOAD_MAX_THREADS=4
fi

# Validate DOWNLOAD_MAX_THREADS is reasonable
if [[ "${DOWNLOAD_MAX_THREADS}" -lt 1 ]]; then
 DOWNLOAD_MAX_THREADS=1
elif [[ "${DOWNLOAD_MAX_THREADS}" -gt 8 ]]; then
 DOWNLOAD_MAX_THREADS=8
fi

# Cleanup configuration
# Controls whether temporary files and directories should be cleaned up after processing
# Set to false to preserve files for debugging purposes
declare CLEAN="${CLEAN:-false}"

# XML Validation configuration
# Skip XML validation for faster processing when using trusted Planet dumps
# Set to false to enable full validation (structure, dates, coordinates)
# Set to true to skip all validations and assume XML is well-formed (FASTER)
# Default: true (skip validation for speed, assuming official OSM Planet is valid)
# WARNING: Only skip validation if you trust the XML source (e.g., official OSM Planet)
declare SKIP_XML_VALIDATION="${SKIP_XML_VALIDATION:-true}"

# CSV Validation configuration
# Skip CSV validation for faster processing when CSV files are generated correctly
# Set to false to enable CSV validation (structure, enum compatibility)
# Set to true to skip all CSV validations (FASTER)
# Default: true (skip validation for speed, PostgreSQL will validate on COPY)
# WARNING: PostgreSQL COPY will validate enums and structure anyway, so pre-validation
# is redundant for production. Enable only for debugging CSV generation issues.
declare SKIP_CSV_VALIDATION="${SKIP_CSV_VALIDATION:-true}"

# Overpass API rate limiting
# Maximum number of concurrent downloads from Overpass API
# Overpass has 2 servers Ã— 4 slots = 8 total concurrent slots
# shellcheck disable=SC2034
declare RATE_LIMIT="${RATE_LIMIT:-8}"

# Assignment chunk size for geolocation queue (notes per batch)
# Number of notes processed per batch when assigning countries
# shellcheck disable=SC2034
declare ASSIGN_CHUNK_SIZE="${ASSIGN_CHUNK_SIZE:-5000}"

# Verification configuration for note location integrity checks
# Verification chunk size (notes per batch) for parallel processing
# Larger chunks reduce overhead but increase memory usage per thread
# shellcheck disable=SC2034
declare VERIFY_CHUNK_SIZE="${VERIFY_CHUNK_SIZE:-100000}"

# SQL sub-chunk size within each verification chunk
# Number of notes processed per SQL query during integrity verification
# Larger batches reduce query overhead but may increase memory usage per query
# Production default: 20000 (optimized for performance with large datasets)
# shellcheck disable=SC2034
declare VERIFY_SQL_BATCH_SIZE="${VERIFY_SQL_BATCH_SIZE:-20000}"

# Parallel threads for verification
# Number of threads to use for parallel integrity verification
# shellcheck disable=SC2034
declare VERIFY_THREADS="${VERIFY_THREADS:-}"

# Batch size for reassigning notes when country boundaries change
# Number of notes processed per batch with partial commits
# Smaller batches (e.g., 1000) provide better progress visibility and lower memory usage
# Larger batches (e.g., 10000) reduce transaction overhead but increase lock duration
# Default: 1000 notes per batch
# shellcheck disable=SC2034
declare REASSIGN_NOTES_BATCH_SIZE="${REASSIGN_NOTES_BATCH_SIZE:-1000}"

# Check for missing maritime boundaries after processing
# Compares database with OSM (queries Overpass API for all maritime boundaries)
# Set to true to enable check, false to disable
# Default: false (disabled)
# shellcheck disable=SC2034
declare CHECK_MISSING_MARITIMES="${CHECK_MISSING_MARITIMES:-false}"


