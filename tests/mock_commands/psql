#!/bin/bash

# Mock psql command for testing
# Author: Andres Gomez (AngocA)
# Version: 2025-12-07

# Function to simulate database operations
mock_database_operation() {
 local operation="$1"
 local args="$2"
 
 case "$operation" in
  -d)
   # Database connection
   # In quiet mode (-q), don't output connection message to stdout
   # This prevents interference when capturing command output
   if [[ "${QUIET_MODE:-false}" != "true" ]]; then
     echo "Connected to database: $args"
   fi
   ;;
  -c)
   # SQL command
   # IMPORTANT: Check for COPY commands FIRST, before SELECT
   # The SQL may contain both COPY and SELECT commands, but COPY must be processed
   # Handle COPY ... TO commands that generate CSV files
   # The SQL may contain multiple COPY commands, so we need to process all of them
   # Note: We check for "COPY" followed by "TO" to ensure we're matching COPY commands,
   # not SELECT statements that might be inside COPY subqueries
   # Normalize SQL first to handle multi-line format
   local sql_check
   sql_check=$(echo "$args" | tr '\n' ' ' | tr '\r' ' ' | sed 's/[[:space:]]\+/ /g')
   
   # Debug: log what we're checking (only in test mode)
   if [[ "${TEST_MODE:-}" == "true" ]]; then
     echo "Mock psql: Checking SQL for COPY commands (length: ${#sql_check})" >&2
     echo "Mock psql: First 200 chars: ${sql_check:0:200}..." >&2
   fi
   
   # Check if SQL contains COPY ... TO pattern
   if echo "$sql_check" | grep -q "COPY.*TO" 2>/dev/null; then
     # Always log when COPY is detected (for debugging)
     echo "Mock psql: COPY.*TO pattern detected, processing COPY commands" >&2
     echo "Mock psql: SQL length: ${#sql_check}, first 300 chars: ${sql_check:0:300}..." >&2
     # Function to create CSV file based on path
     create_csv_file() {
       local filepath="$1"
       
       # Create directory if it doesn't exist
       local filedir
       filedir=$(dirname "$filepath")
       if [[ -n "$filedir" ]] && [[ "$filedir" != "." ]]; then
         mkdir -p "$filedir" 2>/dev/null || true
       fi
       
       # Create CSV file with header based on file name
       if [[ "$filepath" == *"lastNote"* ]] || [[ "$filepath" == *"LAST_NOTE"* ]]; then
         echo "note_id,latitude,longitude,created_at,status,closed_at" > "$filepath"
         echo "123,40.7128,-74.0060,2023-01-01 00:00:00,open," >> "$filepath"
       elif [[ "$filepath" == *"lastComment"* ]] || [[ "$filepath" == *"LAST_COMMENT"* ]]; then
         echo "comment_id,note_id,sequence_action,created_at,action,user_id,username" > "$filepath"
         echo "456,123,1,2023-01-01 00:00:00,opened,12345,testuser" >> "$filepath"
       elif [[ "$filepath" == *"differentNoteIds"* ]] || [[ "$filepath" == *"DIFFERENT_NOTE_IDS"* ]]; then
         echo "note_id,latitude,longitude,created_at,status,closed_at" > "$filepath"
         # Empty file (no differences) - just header
       elif [[ "$filepath" == *"differentCommentIds"* ]] || [[ "$filepath" == *"DIFFERENT_COMMENT_IDS"* ]]; then
         echo "comment_id,note_id,sequence_action,created_at,action,user_id,username" > "$filepath"
         # Empty file (no differences) - just header
       elif [[ "$filepath" == *"differentNotes"* ]] || [[ "$filepath" == *"DIFFERENT_NOTES"* ]] || [[ "$filepath" == *"DIRRERENT_NOTES"* ]]; then
         echo "note_id,latitude,longitude,created_at,status,closed_at" > "$filepath"
         # Empty file (no differences) - just header
       elif [[ "$filepath" == *"differentNoteComments"* ]] || [[ "$filepath" == *"DIFFERENT_COMMENTS"* ]] || [[ "$filepath" == *"DIRRERENT_COMMENTS"* ]]; then
         echo "comment_id,note_id,sequence_action,created_at,action,user_id,username" > "$filepath"
         # Empty file (no differences) - just header
       elif [[ "$filepath" == *"differentTextComments"* ]] || [[ "$filepath" == *"DIFFERENT_TEXT_COMMENTS"* ]]; then
         echo "text_comment_id,note_id,sequence_action,text" > "$filepath"
         # Empty file (no differences) - just header
       elif [[ "$filepath" == *"textComments"* ]] || [[ "$filepath" == *"DIFFERENCES_TEXT_COMMENT"* ]]; then
         echo "qty,note_id,sequence_action" > "$filepath"
         # Empty file (no differences) - just header
       else
         # Generic CSV file
         echo "id,value" > "$filepath"
       fi
     }
     
     # Process all COPY ... TO commands in the SQL
     # The SQL may have multi-line format, so we need to normalize it first
     # Normalize SQL: replace newlines with spaces and collapse multiple spaces
     local sql_normalized
     sql_normalized=$(echo "$args" | tr '\n' ' ' | tr '\r' ' ' | sed 's/[[:space:]]\+/ /g')
     
     # Extract all file paths from COPY ... TO 'path' patterns
     local temp_file
     temp_file=$(mktemp)
     
     # Extract all TO 'path' patterns from the normalized SQL
     # Use multiple methods to ensure we catch all paths
     
     # Method 1: Extract paths with single quotes followed by WITH (most common pattern)
     echo "$sql_normalized" | sed -n "s/.*TO[[:space:]]*'\\([^']*\\)'[[:space:]]*WITH.*/\\1/p" >> "$temp_file" 2>/dev/null || true
     
     # Method 2: Extract paths with single quotes followed by semicolon
     echo "$sql_normalized" | sed -n "s/.*TO[[:space:]]*'\\([^']*\\)'[[:space:]]*;.*/\\1/p" >> "$temp_file" 2>/dev/null || true
     
     # Method 3: Extract paths with double quotes
     echo "$sql_normalized" | sed -n 's/.*TO[[:space:]]*"\([^"]*\)"[[:space:]]*WITH.*/\1/p' >> "$temp_file" 2>/dev/null || true
     echo "$sql_normalized" | sed -n 's/.*TO[[:space:]]*"\([^"]*\)"[[:space:]]*;.*/\1/p' >> "$temp_file" 2>/dev/null || true
     
     # Method 4: Split by COPY and extract from each COPY block (most reliable)
     # This handles multi-line SQL better and handles cases where COPY is on separate line
     {
       # Use a more robust approach: find all TO 'path' patterns directly
       # This avoids issues with splitting by COPY which can capture too much
       # Extract all paths that appear after "TO" keyword
       echo "$sql_normalized" | grep -oE "TO[[:space:]]+['\"][^'\"]+['\"]" | sed -E "s/TO[[:space:]]+['\"]([^'\"]+)['\"]/\\1/" >> "$temp_file" 2>/dev/null || true
       
       # Also try splitting by COPY blocks as fallback
       echo "$sql_normalized" | sed 's/COPY[[:space:]]*(/\n---COPY---/g' | sed 's/COPY[[:space:]]\+/\n---COPY---/g' | grep "^---COPY---" | while IFS= read -r copy_block; do
         # Extract path with single quotes from this COPY block
         # Look for TO 'path' pattern, handling WITH clause and semicolon
         echo "$copy_block" | sed -n "s/.*TO[[:space:]]*'\\([^']*\\)'[[:space:]]*WITH.*/\\1/p"
         echo "$copy_block" | sed -n "s/.*TO[[:space:]]*'\\([^']*\\)'[[:space:]]*;.*/\\1/p"
         # Extract path with double quotes from this COPY block
         echo "$copy_block" | sed -n 's/.*TO[[:space:]]*"\([^"]*\)"[[:space:]]*WITH.*/\1/p'
         echo "$copy_block" | sed -n 's/.*TO[[:space:]]*"\([^"]*\)"[[:space:]]*;.*/\1/p'
       done >> "$temp_file" 2>/dev/null || true
     }
     
     # Method 5: Simple extraction - find all TO 'path' patterns (last resort)
     # This catches any remaining patterns, including those with extra whitespace
     echo "$sql_normalized" | grep -oE "TO[[:space:]]+['\"][^'\"]+['\"]" | sed -E "s/TO[[:space:]]+['\"]([^'\"]+)['\"]/\\1/" >> "$temp_file" 2>/dev/null || true
     
     # Remove duplicates and process each unique path
     local processed_paths=()
     while IFS= read -r filepath; do
       # Clean up the path (remove leading/trailing whitespace)
       filepath=$(echo "$filepath" | sed 's/^[[:space:]]*//' | sed 's/[[:space:]]*$//')
       
       # Skip empty paths
       [[ -z "$filepath" ]] && continue
       
       # Skip paths that contain SQL keywords or are clearly not file paths
       # These indicate we captured too much text
       if [[ "$filepath" == *"SELECT"* ]] || [[ "$filepath" == *"FROM"* ]] || [[ "$filepath" == *"WHERE"* ]] || \
          [[ "$filepath" == *"COPY"* ]] || [[ "$filepath" == *"--"* ]] || [[ "$filepath" == *"/*"* ]]; then
         continue
       fi
       
       # Skip paths with variables not substituted (but allow /tmp paths)
       if [[ "$filepath" == *'$'* ]] && [[ "$filepath" != *"/tmp"* ]]; then
         continue
       fi
       
       # Remove WITH clause if present (shouldn't be there, but just in case)
       filepath=$(echo "$filepath" | sed 's/[[:space:]]*WITH.*$//')
       filepath=$(echo "$filepath" | sed 's/[[:space:]]*;.*$//')
       
       # Check if we've already processed this path
       local already_processed=0
       for processed in "${processed_paths[@]}"; do
         if [[ "$processed" == "$filepath" ]]; then
           already_processed=1
           break
         fi
       done
       
       if [[ $already_processed -eq 0 ]]; then
         processed_paths+=("$filepath")
         create_csv_file "$filepath"
         # Always log CSV creation (for debugging)
         echo "Mock psql: Created CSV file: $filepath" >&2
       fi
     done < "$temp_file"
     
     rm -f "$temp_file"
     
     # Always show what was processed (for debugging)
     if [[ ${#processed_paths[@]} -gt 0 ]]; then
       echo "Mock psql: Processed ${#processed_paths[@]} COPY commands" >&2
       for path in "${processed_paths[@]}"; do
         echo "Mock psql:   - $path" >&2
       done
     else
       echo "Mock psql: WARNING - No paths were processed from COPY commands!" >&2
     fi
     
     # Output result for each COPY command found
     # PostgreSQL outputs one COPY result per command
     local copy_count=${#processed_paths[@]}
     if [[ $copy_count -gt 0 ]]; then
       # Output one COPY result per file created
       for filepath in "${processed_paths[@]}"; do
         local lines
         lines=$(wc -l < "$filepath" 2>/dev/null || echo 1)
         echo "COPY $lines"
       done
     else
       echo "COPY 0"
     fi
     # Exit early after processing COPY commands - don't process SELECT or other commands
     # This ensures COPY commands are handled even if SQL also contains SELECT
     # We've already output the COPY results, so we're done - exit the case with ;;
   else
     # If we get here, it means COPY was not processed, so handle other SQL commands
     if [[ "$sql_check" == *"SELECT"* ]]; then
       if [[ "${TEST_MODE:-}" == "true" ]]; then
         echo "Mock psql: No COPY commands found, processing SELECT" >&2
       fi
       # Check for PostGIS version function
       if [[ "$sql_check" == *"PostGIS_version"* ]]; then
         # PostGIS is available in test environment
         echo "3.3.0"
       # Check for extension queries
       elif [[ "$sql_check" == *"pg_extension"* ]] && [[ "$sql_check" == *"extname"* ]]; then
         # Check which extension is being queried
         if [[ "$sql_check" == *"btree_gist"* ]]; then
           # btree_gist extension exists in test environment
           echo "1"
         elif [[ "$sql_check" == *"postgis"* ]]; then
           # PostGIS extension exists in test environment
           echo "1"
         else
           # Other extension - assume it exists
           echo "1"
         fi
       elif [[ "$sql_check" == *"COUNT"* ]]; then
         # Default COUNT result
         echo "100"
       elif [[ "$sql_check" == *"TABLE_NAME"* ]]; then
         echo "notes"
         echo "note_comments"
         echo "countries"
         echo "logs"
         echo "tries"
       else
         echo "1|test|2023-01-01"
       fi
     elif [[ "$sql_check" == *"CREATE"* ]]; then
       echo "CREATE TABLE"
     elif [[ "$sql_check" == *"INSERT"* ]]; then
       echo "INSERT 0 1"
     elif [[ "$sql_check" == *"UPDATE"* ]]; then
       echo "UPDATE 1"
     elif [[ "$sql_check" == *"DELETE"* ]]; then
       echo "DELETE 1"
     elif [[ "$sql_check" == *"DROP"* ]]; then
       echo "DROP TABLE"
     elif [[ "$sql_check" == *"VACUUM"* ]]; then
       echo "VACUUM"
     elif [[ "$sql_check" == *"ANALYZE"* ]]; then
       echo "ANALYZE"
     else
       echo "OK"
     fi
   fi
   ;;
  -f)
   # SQL file
   if [[ -f "$args" ]]; then
     echo "Executing SQL file: $args"
     
     # Read SQL file and check for \copy commands
     local sql_content
     sql_content=$(cat "$args" 2>/dev/null || echo "")
     local sql_normalized
     sql_normalized=$(echo "$sql_content" | tr '\n' ' ' | tr '\r' ' ' | sed 's/[[:space:]]\+/ /g')
     
     # Check for \copy commands (client-side COPY)
     # \copy can appear as \copy or \\copy in the SQL
     # Check both raw content and normalized content for better detection
     # Also check for patterns that might be in single-line format after conversion
     if echo "$sql_content" | grep -qE "^[[:space:]]*\\\\copy|^[[:space:]]*\\\copy|[[:space:]]+\\\\copy|[[:space:]]+\\\copy" 2>/dev/null || echo "$sql_normalized" | grep -qE "(\\\\copy|\\\copy)" 2>/dev/null; then
       # Extract file paths from \copy commands using a more robust method
       # Process the normalized SQL to extract all file paths
       local temp_file
       temp_file=$(mktemp)
       
       # Extract all TO 'path' patterns from normalized SQL (single-line format after conversion)
       # This handles the format: \copy (SELECT ...) TO '/path/file.csv' WITH ...
       # Extract all occurrences, including those with variables like ${VAR} or $VAR
       echo "$sql_normalized" | grep -oE "TO[[:space:]]+['\"][^'\"]+['\"]" | sed -E "s/TO[[:space:]]+['\"]([^'\"]+)['\"]/\\1/" >> "$temp_file" 2>/dev/null || true
       
       # Also try extracting from raw content in case normalization missed something
       echo "$sql_content" | grep -oE "TO[[:space:]]+['\"][^'\"]+['\"]" | sed -E "s/TO[[:space:]]+['\"]([^'\"]+)['\"]/\\1/" >> "$temp_file" 2>/dev/null || true
       
       # If we found variables like ${VAR}, try to resolve them from environment
       # This handles cases where variables weren't substituted before conversion
       if grep -qE '\$\{' "$temp_file" 2>/dev/null; then
         # Try to resolve variables from environment
         local resolved_file
         resolved_file=$(mktemp)
         while IFS= read -r filepath; do
           # If path contains ${VAR}, try to resolve it
           if echo "$filepath" | grep -qE '\$\{'; then
             # Try to get the variable name and resolve it
             local var_name
             var_name=$(echo "$filepath" | sed -E "s/.*\\\$\{([^}]+)\}.*/\\1/")
             if [[ -n "${var_name}" ]] && [[ -n "${!var_name:-}" ]]; then
               echo "${!var_name}" >> "$resolved_file"
             else
               # Keep original if can't resolve
               echo "$filepath" >> "$resolved_file"
             fi
           else
             echo "$filepath" >> "$resolved_file"
           fi
         done < "$temp_file"
         mv "$resolved_file" "$temp_file"
       fi
       
       # Remove duplicates and empty lines
       sort -u "$temp_file" | grep -v "^[[:space:]]*$" > "${temp_file}.clean" 2>/dev/null || true
       mv "${temp_file}.clean" "$temp_file" 2>/dev/null || true
       
       # Process each file path found
       while IFS= read -r filepath; do
         filepath=$(echo "$filepath" | sed 's/^[[:space:]]*//' | sed 's/[[:space:]]*$//')
         [[ -z "$filepath" ]] && continue
         
         # Create directory if needed
         local filedir
         filedir=$(dirname "$filepath")
         if [[ -n "$filedir" ]] && [[ "$filedir" != "." ]]; then
           mkdir -p "$filedir" 2>/dev/null || true
         fi
         
         # Create CSV file based on filename pattern
         if [[ "$filepath" == *"lastNote"* ]] || [[ "$filepath" == *"LAST_NOTE"* ]]; then
           echo "note_id,latitude,longitude,created_at,status,closed_at" > "$filepath"
           echo "123,40.7128,-74.0060,2023-01-01 00:00:00,open," >> "$filepath"
         elif [[ "$filepath" == *"lastComment"* ]] || [[ "$filepath" == *"LAST_COMMENT"* ]]; then
           echo "comment_id,note_id,sequence_action,created_at,action,user_id,username" > "$filepath"
           echo "456,123,1,2023-01-01 00:00:00,opened,12345,testuser" >> "$filepath"
         elif [[ "$filepath" == *"differentNoteIds"* ]] || [[ "$filepath" == *"DIFFERENT_NOTE_IDS"* ]]; then
           echo "note_id" > "$filepath"
           # Empty file (no differences) - just header
         elif [[ "$filepath" == *"differentCommentIds"* ]] || [[ "$filepath" == *"DIFFERENT_COMMENT_IDS"* ]]; then
           echo "comment_id" > "$filepath"
           # Empty file (no differences) - just header
         elif [[ "$filepath" == *"differentNotes"* ]] || [[ "$filepath" == *"DIFFERENT_NOTES"* ]]; then
           echo "note_id,latitude,longitude,created_at,status,closed_at" > "$filepath"
           # Empty file (no differences) - just header
         elif [[ "$filepath" == *"differentTextComments"* ]] || [[ "$filepath" == *"DIFFERENT_TEXT_COMMENTS"* ]]; then
           echo "text_comment_id" > "$filepath"
           # Empty file (no differences) - just header
         elif [[ "$filepath" == *"textComments"* ]] || [[ "$filepath" == *"DIFFERENCES_TEXT_COMMENT"* ]]; then
           echo "qty,note_id,sequence_action" > "$filepath"
           # Empty file (no differences) - just header
         else
           # Generic CSV file
           echo "id,value" > "$filepath"
         fi
       done < "$temp_file"
       
      rm -f "$temp_file"
    fi
    
    # Check if SQL contains RAISE EXCEPTION or error patterns that indicate missing tables
    if echo "$sql_content" | grep -qi "RAISE EXCEPTION" || echo "$sql_normalized" | grep -qi "RAISE EXCEPTION"; then
      if echo "$sql_content" | grep -qiE "Base tables are missing|checkBaseTables|functionsProcess_11_checkBaseTables" || \
         echo "$sql_normalized" | grep -qiE "Base tables are missing|checkBaseTables|functionsProcess_11_checkBaseTables"; then
        local real_psql_path
        real_psql_path=$(command -v /usr/bin/psql 2>/dev/null || echo "")
        if [[ -n "$real_psql_path" ]]; then
          local tables_exist
          tables_exist=$("$real_psql_path" -d "${DATABASE:-notes}" -tAq -c "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public' AND table_name IN ('countries', 'notes', 'note_comments', 'logs');" 2>/dev/null | grep -E '^[0-9]+$' | head -1 || echo "0")
          if [[ "${tables_exist}" -lt "4" ]]; then
            echo "psql:${args}:58: ERROR:  Base tables are missing: notes." >&2
            echo "CONTEXTO:  función PL/pgSQL inline_code_block en la línea 24 en RAISE" >&2
            exit 3
          fi
        else
          # Fallback for environments without real psql (e.g., pure mock)
          if [[ -n "${HYBRID_MOCK_MODE:-}" ]] || [[ -n "${TEST_MODE:-}" ]]; then
            echo "psql:${args}:58: ERROR:  Base tables are missing: notes." >&2
            echo "CONTEXTO:  función PL/pgSQL inline_code_block en la línea 24 en RAISE" >&2
            exit 3
          fi
        fi
      fi
    fi
    
    echo "OK"
  else
    echo "ERROR: File not found: $args" >&2
    exit 1
  fi
  ;;
  -l)
   # List databases
   # Output format: name|owner|encoding|collate|ctype|access privileges
   # When used with -qt (quiet, tuples only), only names are shown
   # Include common test databases
   # Check if we're in quiet/tuples mode (set by parser)
   local db_list=""
   # Include test database if DBNAME is set in environment
   if [[ -n "${DBNAME:-}" ]]; then
     db_list="${DBNAME}"
   else
     # Default test database names
     db_list="osm-notes-test
osm-notes"
   fi
   
   # Add standard PostgreSQL databases
   db_list="postgres
template0
template1
${db_list}"
   
   # Output based on mode
   if [[ "${TUPLES_ONLY:-false}" == "true" ]]; then
     # Tuples only mode - just database names
     echo "$db_list"
   else
     # Full format: name|owner|encoding|collate|ctype|access privileges
     echo "$db_list" | while IFS= read -r dbname; do
       if [[ "$dbname" == "template0" ]] || [[ "$dbname" == "template1" ]]; then
         echo "${dbname}|postgres|UTF8|en_US.UTF-8|en_US.UTF-8|=c/postgres"
       else
         echo "${dbname}|${USER:-angoca}|UTF8|en_US.UTF-8|en_US.UTF-8|"
       fi
     done
   fi
   ;;
  -v)
   # Variable assignment
   echo "Variable set: $args"
   ;;
  --version)
   echo "psql (PostgreSQL) 15.1"
   exit 0
   ;;
  *)
   echo "Unknown operation: $operation $args"
   ;;
 esac
}

# Parse arguments
ARGS=()
DATABASE=""
COMMAND=""
FILE=""
VARIABLES=()
LIST_DATABASES=false
QUIET_MODE=false
TUPLES_ONLY=false

while [[ $# -gt 0 ]]; do
 case $1 in
  -d)
   DATABASE="$2"
   shift 2
   ;;
  -c)
   COMMAND="$2"
   shift 2
   ;;
  -f)
   FILE="$2"
   shift 2
   ;;
  -l|-lq|-lt|-lqt)
   # List databases flag - handle combined flags
   LIST_DATABASES=true
   if [[ "$1" == *"q"* ]]; then
     QUIET_MODE=true
   fi
   if [[ "$1" == *"t"* ]]; then
     TUPLES_ONLY=true
   fi
   shift
   ;;
  -q)
   QUIET_MODE=true
   shift
   ;;
  -t)
   TUPLES_ONLY=true
   shift
   ;;
  -qt)
   QUIET_MODE=true
   TUPLES_ONLY=true
   shift
   ;;
  -*)
   # Handle flag combinations that might include -q (quiet) or -t (tuples only)
   # This catches -Atq, -Aqt, -tAq, and any other combinations
   if [[ "$1" == *"q"* ]]; then
     QUIET_MODE=true
   fi
   if [[ "$1" == *"t"* ]]; then
     TUPLES_ONLY=true
   fi
   shift
   ;;
  -v)
   VARIABLES+=("$2")
   shift 2
   ;;
  -U)
   # User flag - skip the value
   shift 2
   ;;
  --version)
   echo "psql (PostgreSQL) 15.1"
   exit 0
   ;;
  -*)
   # Skip other options
   shift
   ;;
  *)
   ARGS+=("$1")
   shift
   ;;
 esac
done

# Process variables first
for var in "${VARIABLES[@]}"; do
 mock_database_operation "-v" "$var"
done

# Process list databases if requested
# Export flags so mock_database_operation can use them
export TUPLES_ONLY QUIET_MODE
if [[ "${LIST_DATABASES:-false}" == "true" ]]; then
 mock_database_operation "-l" ""
fi

# Process main operation
if [[ -n "$DATABASE" ]]; then
 mock_database_operation "-d" "$DATABASE"
fi

if [[ -n "$COMMAND" ]]; then
 mock_database_operation "-c" "$COMMAND"
elif [[ -n "$FILE" ]]; then
 mock_database_operation "-f" "$FILE"
elif [[ -t 0 ]]; then
 # No stdin available, nothing to do
 :
else
 # Check if we're being used as a coprocess (stdin is a pipe, not terminal)
 # When used with coproc, we need to stay alive to receive multiple SQL commands.
 # Commands are sent by writing to stdin, and we process them one by one.
 # 
 # IMPORTANT: We add a small initial delay to ensure coproc sets up the PID correctly
 # before we start processing. This prevents the "PID not set" issue when commands
 # terminate too quickly.
 
 # Small delay to ensure coproc PID is established (critical for coproc to work)
 sleep 0.01 2>/dev/null || true
 
 # Read and process SQL commands continuously until stdin is closed
 while IFS= read -r line || [[ -n "${line:-}" ]]; do
   # Accumulate multi-line SQL commands
   if [[ -z "${sql_block:-}" ]]; then
     sql_block="$line"
   else
     sql_block="$sql_block
$line"
   fi
   
   # Process when we have content (simple heuristic: if line ends or is complete)
   # For coprocess usage, each SQL command is typically sent as a complete block
   if [[ -n "${sql_block// /}" ]]; then
     mock_database_operation "-c" "$sql_block"
     sql_block=""  # Reset for next command
   fi
 done
 
 # Process any remaining content
 if [[ -n "${sql_block:-}" ]] && [[ -n "${sql_block// /}" ]]; then
   mock_database_operation "-c" "$sql_block"
 fi
fi

exit 0
